{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tracemalloc\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score , recall_score\n",
    "\n",
    "# Path to the dataset\n",
    "train_path = './assets/train/'\n",
    "test_path = './assets/test/'\n",
    "\n",
    "# Feature extractor (SIFT)\n",
    "sift = cv2.SIFT_create(contrastThreshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptors_from_image(image_path):\n",
    "    \"\"\"\n",
    "    Extract exactly a specified number of descriptors from an image using SIFT.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image not found or invalid: {image_path}\")\n",
    "    \n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    \n",
    "    if descriptors is None or len(descriptors) == 0:\n",
    "        raise ValueError(f\"No descriptors found in image: {image_path}\")\n",
    "\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_descriptors_all(data_path):\n",
    "    \"\"\"\n",
    "    Collect descriptors and their corresponding labels.\n",
    "    \"\"\"\n",
    "    all_descriptors = []\n",
    "    all_labels = []\n",
    "    \n",
    "    classes = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "    \n",
    "    num_des_tot = 0\n",
    "    num_des_class_dist = []\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_folder = os.path.join(data_path, class_name)\n",
    "        # print(f\"Processing class: {class_name}\")\n",
    "        \n",
    "        num_des_class = 0\n",
    "        for image_name in os.listdir(class_folder):\n",
    "            if image_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                image_path = os.path.join(class_folder, image_name)\n",
    "                try:\n",
    "                    # Extract descriptors for this image\n",
    "                    descriptors = extract_descriptors_from_image(image_path)\n",
    "                    \n",
    "                    # Append descriptors and corresponding labels\n",
    "                    all_descriptors += descriptors.tolist()\n",
    "                    all_labels += [i]*len(descriptors)\n",
    "                    num_des_class += len(descriptors)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error processing {image_name}: {e}\")\n",
    "        num_des_tot += num_des_class\n",
    "        num_des_class_dist.append(num_des_class)\n",
    "    print(f'total number of descriptors: {num_des_tot}')\n",
    "    print(f'number of descriptors per class: {num_des_class_dist}')\n",
    "    \n",
    "    # Convert to NumPy arrays\n",
    "    all_descriptors = np.array(all_descriptors)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    return all_descriptors, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_descriptors_with_labels(data_path):\n",
    "    \"\"\"\n",
    "    Collect descriptors and their corresponding labels.\n",
    "    \"\"\"\n",
    "    all_descriptors = []\n",
    "    all_labels = []\n",
    "    \n",
    "    classes = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "\n",
    "    num_des_tot = 0\n",
    "    num_des_class_dist = []\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_folder = os.path.join(data_path, class_name)\n",
    "        # print(f\"Processing class: {class_name}\")\n",
    "        \n",
    "        for image_name in os.listdir(class_folder):\n",
    "            if image_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                image_path = os.path.join(class_folder, image_name)\n",
    "                try:\n",
    "                    # Extract descriptors for this image\n",
    "                    descriptors = extract_descriptors_from_image(image_path)\n",
    "                    all_descriptors.append(descriptors.tolist())\n",
    "                    all_labels.append(i)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error processing {image_name}: {e}\")\n",
    "    \n",
    "    # Convert to NumPy arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    return all_descriptors, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of descriptors: 104008\n",
      "number of descriptors per class: [13465, 15119, 5421, 8680, 10758, 14686, 18524, 8025, 2550, 6780]\n",
      "Descriptor extraction completed.\n"
     ]
    }
   ],
   "source": [
    "# Extract descriptors and labels\n",
    "descriptors_pool, labels_pool = collect_descriptors_all(train_path)\n",
    "print(\"Descriptor extraction completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. K-means codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct K-Means codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (k)\n",
    "k = 30\n",
    "\n",
    "# Initialize and fit KMeans\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, verbose=0, n_init=10)\n",
    "kmeans.fit(descriptors_pool)\n",
    "\n",
    "# Access cluster centers and labels\n",
    "k_means_codebook = kmeans.cluster_centers_  # Shape: (k, 128)\n",
    "labels = kmeans.labels_  # Shape: (104008,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_histogram(img_descriptors, kmeans):\n",
    "    # Predict the closest cluster for each descriptor\n",
    "    labels = kmeans.predict(img_descriptors)  # This gives the cluster index for each descriptor\n",
    "    \n",
    "    # Create a histogram of cluster assignments (BoW)\n",
    "    hist, _ = np.histogram(labels, bins=np.arange(k+1), range=(0, k))\n",
    "    \n",
    "    # Normalize the histogram to get a normalized BoW representation\n",
    "    hist = hist / np.sum(hist)  # Normalize to make it a probability distribution\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply K-means codebook for train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor extraction completed.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'perf_counter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m train_descriptors, train_labels \u001b[38;5;241m=\u001b[39m collect_descriptors_with_labels(train_path)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescriptor extraction completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperf_counter\u001b[49m()\n\u001b[0;32m      7\u001b[0m train_bow \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m descriptors \u001b[38;5;129;01min\u001b[39;00m train_descriptors:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'perf_counter'"
     ]
    }
   ],
   "source": [
    "# Extract train descriptors and labels\n",
    "train_descriptors, train_labels = collect_descriptors_with_labels(train_path)\n",
    "print(\"Descriptor extraction completed.\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "train_bow = []\n",
    "for descriptors in train_descriptors:\n",
    "    image_bow = create_bow_histogram(descriptors, kmeans)\n",
    "    train_bow.append(image_bow)\n",
    "\n",
    "train_bow = np.array(train_bow)\n",
    "\n",
    "vq_time = time.perf_counter() - start_time\n",
    "print(f'vector quantization time for train data: {vq_time}')\n",
    "\n",
    "x_train = train_bow\n",
    "y_train = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor extraction completed.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'perf_counter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m test_descriptors, test_labels \u001b[38;5;241m=\u001b[39m collect_descriptors_with_labels(test_path)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescriptor extraction completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperf_counter\u001b[49m()\n\u001b[0;32m      7\u001b[0m test_bow \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m descriptors \u001b[38;5;129;01min\u001b[39;00m test_descriptors:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'perf_counter'"
     ]
    }
   ],
   "source": [
    "# Extract test descriptors and labels\n",
    "test_descriptors, test_labels = collect_descriptors_with_labels(test_path)\n",
    "print(\"Descriptor extraction completed.\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "test_bow = []\n",
    "for descriptors in test_descriptors:\n",
    "    image_bow = create_bow_histogram(descriptors, kmeans)\n",
    "    test_bow.append(image_bow)\n",
    "\n",
    "test_bow = np.array(test_bow)\n",
    "\n",
    "time = time.perf_counter() - start_time\n",
    "print(f'vector quantization time for train data: {time}')\n",
    "\n",
    "x_test = test_bow\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. RF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classification(x_train, y_train, x_test, n_estimators=30, max_depth=10, bootstrap=True, random_state=None, max_samples=0.7, max_features=\"sqrt\", criterion='entropy'):\n",
    "    rf_clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, bootstrap=bootstrap, random_state=random_state, max_samples=max_samples, max_features=max_features, criterion=criterion)\n",
    "    # Train ----------------------------------------------------------\n",
    "    tracemalloc.start() \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    rf_clf.fit(x_train, y_train.ravel())\n",
    "    \n",
    "    train_time = time.perf_counter() - start_time\n",
    "    current, train_peak_memory = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    y_train_pred = rf_clf.predict(x_train)\n",
    "\n",
    "    # Test ----------------------------------------------------------\n",
    "    tracemalloc.start() \n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    y_test_pred = rf_clf.predict(x_test)\n",
    "\n",
    "    test_time = time.perf_counter()- start_time\n",
    "    current, test_peak_memory = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    # Retrieve the maximum depth of each tree in the forest\n",
    "    tree_depths = [estimator.tree_.max_depth for estimator in rf_clf.estimators_]\n",
    "    max_tree_depth = max(tree_depths)\n",
    "\n",
    "    return y_train_pred, y_test_pred, train_time, test_time, train_peak_memory, test_peak_memory, max_tree_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak learner: Axis-aligned test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.1911473000072874\n",
      "train accuracy: 0.9733333333333334\n",
      "\n",
      "\n",
      "test time: 0.004937999998219311\n",
      "test_accuracy: 0.46\n",
      "\n",
      "\n",
      "test_precision: 0.45568392134181607\n",
      "test_recall: 0.45999999999999996\n",
      "\n",
      "\n",
      "max tree depth: 5\n"
     ]
    }
   ],
   "source": [
    "y_train_pred, y_test_pred, train_time, test_time, train_peak_memory, test_peak_memory, max_tree_depth = RF_classification(x_train, y_train, x_test, n_estimators=30, max_depth=5, bootstrap=True, random_state=None, max_samples=0.7, max_features=\"sqrt\", criterion='entropy')\n",
    "\n",
    "train_accuracy = accuracy_score(y_train.T, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test.T, y_test_pred)\n",
    "test_precision = precision_score(y_test.T, y_test_pred, average= \"macro\", zero_division=0)\n",
    "test_recall = recall_score(y_test.T, y_test_pred, average= \"macro\", zero_division=0)\n",
    "\n",
    "print(f'train time: {train_time}')\n",
    "print(f'train accuracy: {train_accuracy}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'test time: {test_time}')\n",
    "print(f'test_accuracy: {test_accuracy}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'test_precision: {test_precision}')\n",
    "print(f'test_recall: {test_recall}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'max tree depth: {max_tree_depth}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak learner: Two-pixel test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_two_pixel_features(x_train, x_test, n_pairs=None, random_seed=None):\n",
    "    x_combined = np.concatenate([x_train, x_test], axis=1)\n",
    "\n",
    "    n_features, n_samples = x_combined.shape\n",
    "\n",
    "    # Set random seed if provided\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    # Generate all unique pairs of features where i != j\n",
    "    feature_pairs = [(i, j) for i in range(n_features) for j in range(i + 1, n_features)]\n",
    "\n",
    "    # If n_pairs is specified, randomly select a subset of feature pairs\n",
    "    if n_pairs is not None and n_pairs < len(feature_pairs):\n",
    "        feature_pairs = random.sample(feature_pairs, n_pairs)\n",
    "    \n",
    "    # Initialize a new features matrix for pairwise differences\n",
    "    new_features = np.zeros((len(feature_pairs), n_samples))\n",
    "\n",
    "    # Fill in the new features with pairwise differences\n",
    "    for idx, (i, j) in enumerate(feature_pairs):\n",
    "        new_features[idx, :] = x_combined[i, :] - x_combined[j, :]\n",
    "\n",
    "    x_train_2pix = new_features[:, :x_train.shape[1]]\n",
    "    x_test_2pix = new_features[:, x_train.shape[1]:]\n",
    "\n",
    "    return x_train_2pix.T, x_test_2pix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.19790070003364235\n",
      "train accuracy: 0.9933333333333333\n",
      "\n",
      "\n",
      "test time: 0.002572300028987229\n",
      "test_accuracy: 0.35333333333333333\n",
      "\n",
      "\n",
      "test_precision: 0.34651667776667777\n",
      "test_recall: 0.3533333333333334\n",
      "\n",
      "\n",
      "max tree depth: 10\n"
     ]
    }
   ],
   "source": [
    "n_pairs = x_train.shape[1]\n",
    "# n_pairs = 100\n",
    "x_train_2pix, x_test_2pix = create_two_pixel_features(x_train.T, x_test.T, n_pairs=n_pairs, random_seed=0)\n",
    "y_train_pred, y_test_pred, train_time, test_time, train_peak_memory, test_peak_memory, max_tree_depth = RF_classification(x_train_2pix, y_train, x_test_2pix, n_estimators=30, max_depth=10, bootstrap=True, random_state=None, max_samples=0.7, max_features=\"sqrt\", criterion='entropy')\n",
    "\n",
    "train_accuracy = accuracy_score(y_train.T, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test.T, y_test_pred)\n",
    "test_precision = precision_score(y_test.T, y_test_pred, average= \"macro\", zero_division=0)\n",
    "test_recall = recall_score(y_test.T, y_test_pred, average= \"macro\", zero_division=0)\n",
    "\n",
    "print(f'train time: {train_time}')\n",
    "print(f'train accuracy: {train_accuracy}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'test time: {test_time}')\n",
    "print(f'test_accuracy: {test_accuracy}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'test_precision: {test_precision}')\n",
    "print(f'test_recall: {test_recall}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'max tree depth: {max_tree_depth}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
