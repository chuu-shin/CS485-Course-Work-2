{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Extraction (Dense SIFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dense_sift_features(image_path, step=8, size=16):\n",
    "    \"\"\"\n",
    "    Extract dense SIFT features from an image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        return None\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints = [cv2.KeyPoint(x, y, size) \n",
    "                 for y in range(0, image.shape[0], step) \n",
    "                 for x in range(0, image.shape[1], step)]\n",
    "    _, descriptors = sift.compute(image, keypoints)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the K-means Codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_codebook(data_path, n_clusters=500, sample_size=100000):\n",
    "    \"\"\"\n",
    "    Build a visual vocabulary using K-means clustering.\n",
    "    \"\"\"\n",
    "    all_descriptors = []\n",
    "    class_folders = [os.path.join(data_path, class_dir) for class_dir in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, class_dir))]\n",
    "\n",
    "    for folder in class_folders:\n",
    "        for image_file in tqdm(os.listdir(folder)):\n",
    "            image_path = os.path.join(folder, image_file)\n",
    "            descriptors = extract_dense_sift_features(image_path)\n",
    "            if descriptors is not None:\n",
    "                all_descriptors.append(descriptors)\n",
    "\n",
    "    # Stack all descriptors into a single array\n",
    "    all_descriptors = np.vstack(all_descriptors)\n",
    "    print(f\"Total descriptors: {all_descriptors.shape[0]}\")\n",
    "\n",
    "    # Randomly sample 100k descriptors (or fewer if the dataset is small)\n",
    "    sampled_descriptors = all_descriptors[np.random.choice(all_descriptors.shape[0], min(sample_size, all_descriptors.shape[0]), replace=False)]\n",
    "\n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, verbose=1)\n",
    "    kmeans.fit(sampled_descriptors)\n",
    "    \n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quantize Features and Build Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_and_create_histograms(data_path, kmeans):\n",
    "    \"\"\"\n",
    "    Quantize image features using the codebook and create histograms.\n",
    "    \"\"\"\n",
    "    histograms = []\n",
    "    labels = []\n",
    "\n",
    "    class_folders = [os.path.join(data_path, class_dir) for class_dir in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, class_dir))]\n",
    "\n",
    "    for label, folder in enumerate(class_folders):\n",
    "        for image_file in tqdm(os.listdir(folder)):\n",
    "            image_path = os.path.join(folder, image_file)\n",
    "            descriptors = extract_dense_sift_features(image_path)\n",
    "            if descriptors is None:\n",
    "                continue\n",
    "            \n",
    "            # Quantize descriptors to nearest cluster centers\n",
    "            cluster_assignments = kmeans.predict(descriptors)\n",
    "            \n",
    "            # Create histogram\n",
    "            histogram, _ = np.histogram(cluster_assignments, bins=range(kmeans.n_clusters + 1), density=True)\n",
    "            histograms.append(histogram)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(histograms), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full implementation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_data_path = \"path_to_training_data\"  # Replace with your dataset path\n",
    "    test_data_path = \"path_to_testing_data\"\n",
    "\n",
    "    # Step 1: Build the visual vocabulary\n",
    "    print(\"Building the visual vocabulary...\")\n",
    "    kmeans = build_codebook(train_data_path, n_clusters=500, sample_size=100000)\n",
    "\n",
    "    # Step 2: Quantize features and create histograms\n",
    "    print(\"Creating histograms for training data...\")\n",
    "    train_histograms, train_labels = quantize_and_create_histograms(train_data_path, kmeans)\n",
    "\n",
    "    print(\"Creating histograms for testing data...\")\n",
    "    test_histograms, test_labels = quantize_and_create_histograms(test_data_path, kmeans)\n",
    "\n",
    "    print(f\"Train histograms shape: {train_histograms.shape}\")\n",
    "    print(f\"Test histograms shape: {test_histograms.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
